{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f62e71-8700-4375-bd70-37752f2333a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: False"
     ]
    }
   ],
   "source": [
    "%python\n",
    "\n",
    "\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/allsales', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/budget', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/country', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/customer', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/make', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/marketinginformation', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/model', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/pivottable', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/sales', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/sales2015', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/sales2016', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/sales2017', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/sales2018', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/salesbycountry', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/salescategory', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/salesdetails', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/salestext', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/staff', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/stock', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1b41024-9b3b-40de-be98-e8e07baf49b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%sql\n",
    "\n",
    "USE default;\n",
    "\n",
    "DROP TABLE IF EXISTS allsales;\n",
    "DROP TABLE IF EXISTS budget;\n",
    "DROP TABLE IF EXISTS country;\n",
    "DROP TABLE IF EXISTS customer;\n",
    "DROP TABLE IF EXISTS make;\n",
    "DROP TABLE IF EXISTS marketinginformation;\n",
    "DROP TABLE IF EXISTS model;\n",
    "DROP TABLE IF EXISTS pivottable;\n",
    "DROP TABLE IF EXISTS sales;\n",
    "DROP TABLE IF EXISTS sales2015;\n",
    "DROP TABLE IF EXISTS sales2016;\n",
    "DROP TABLE IF EXISTS sales2017;\n",
    "DROP TABLE IF EXISTS sales2018;\n",
    "DROP TABLE IF EXISTS salesbycountry;\n",
    "DROP TABLE IF EXISTS salescategory;\n",
    "DROP TABLE IF EXISTS salesdetails;\n",
    "DROP TABLE IF EXISTS salestext;\n",
    "DROP TABLE IF EXISTS staff;\n",
    "DROP TABLE IF EXISTS stock;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fb6ecc5-2f2c-4095-92ff-6c1dcfb214dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%python\n",
    "\n",
    "# allsales\n",
    "\n",
    "file_location = \"/FileStore/tables/AllSales.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"AllSales_csv\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"AllSales\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "# Budget\n",
    "\n",
    "file_location = \"/FileStore/tables/Budget.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"Budget_csv\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"Budget\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "# Country\n",
    "\n",
    "file_location = \"/FileStore/tables/Country.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"Country\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"Country\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "# Customer\n",
    "\n",
    "file_location = \"/FileStore/tables/Customer.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"Customer\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"Customer\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "# Make\n",
    "\n",
    "file_location = \"/FileStore/tables/Make.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"Make\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"Make\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "# MarketingInformation\n",
    "\n",
    "file_location = \"/FileStore/tables/MarketingInformation.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"MarketingInformation\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"MarketingInformation\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "# Model\n",
    "\n",
    "file_location = \"/FileStore/tables/Model.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"Model\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"Model\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "# pivottable\n",
    "\n",
    "file_location = \"/FileStore/tables/PivotTable.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"PivotTable\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"pivottable\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "# Sales\n",
    "\n",
    "file_location = \"/FileStore/tables/Sales.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"Sales\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"Sales\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "# Sales2015\n",
    "\n",
    "file_location = \"/FileStore/tables/Sales2015.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"Sales2015\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"Sales2015\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "# Sales2016\n",
    "\n",
    "file_location = \"/FileStore/tables/Sales2016.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"Sales2016\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"Sales2016\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "# Sales2017\n",
    "\n",
    "file_location = \"/FileStore/tables/Sales2017.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"Sales2017\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"Sales2017\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "\n",
    "# Sales2018\n",
    "\n",
    "file_location = \"/FileStore/tables/Sales2018.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"Sales2018\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"Sales2018\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "# SalesByCountry\n",
    "\n",
    "file_location = \"/FileStore/tables/SalesByCountry.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"SalesByCountry_csv\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"SalesByCountry\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "# SalesCategory\n",
    "\n",
    "file_location = \"/FileStore/tables/SalesCategory.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"SalesCategory\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"SalesCategory\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "# SalesDetails\n",
    "\n",
    "file_location = \"/FileStore/tables/SalesDetails.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"SalesDetails\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"SalesDetails\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "# SalesText\n",
    "\n",
    "file_location = \"/FileStore/tables/SalesText.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"SalesText\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"SalesText\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "# Staff\n",
    "\n",
    "file_location = \"/FileStore/tables/Staff.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"Staff\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"Staff\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n",
    "# Stock\n",
    "\n",
    "file_location = \"/FileStore/tables/Stock.csv\"\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .load(file_location)\n",
    "\n",
    "temp_table_name = \"Stock\"\n",
    "df.createOrReplaceTempView(temp_table_name)\n",
    "permanent_table_name = \"Stock\"\n",
    "df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6b4c57f-d2de-40cd-9702-a0e4fe6e91d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 183500122193576,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "LoadData",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
